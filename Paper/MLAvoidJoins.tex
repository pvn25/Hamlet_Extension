\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{amsmath}
\usepackage{graphicx,xspace,verbatim,comment}
\usepackage{hyperref,array,color,balance,multirow}
\usepackage{balance,float,url,amsfonts,alltt}
\usepackage{mathtools,rotating,amsmath,amssymb}
\usepackage{color,ifpdf,fancyvrb,array}
% \usepackage{algorithm,algpseudocode}
\usepackage{etoolbox,listings,subcaption}
\usepackage{bigstrut,morefloats}
%\usepackage[linesnumbered,boxruled]{algorithm2e}
\usepackage[boxruled]{algorithm2e}
\usepackage{pbox}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\newcommand{\eat}[1]{}
\newcommand{\red}{\textcolor{red}}

\newenvironment{packeditems}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packedenums}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\setcopyright{acmcopyright}
% \setcopyright{rightsretained}

\eat{
\acmDOI{10.475/123_4}
\acmISBN{123-4567-24-567/08/06}
}
\acmConference[KDD'17]{KDD Conference}{August 2017}{Halifax, Canada} 
\acmYear{2017}
\copyrightyear{2017}
\acmPrice{15.00}

\title{Stop That Join! Discarding Dimension Tables \\when Learning High Capacity Classifiers}

\author{Vraj Shah}
\affiliation{\institution{University of California, San Diego}}
\email{vps002@eng.ucsd.edu}
\author{Arun Kumar}
\affiliation{\institution{University of California, San Diego}}
\email{arunkk@eng.ucsd.edu}
\author{Xiaojin Zhu}
\affiliation{\institution{University of Wisconsin-Madison}}
\email{jerryzhu@cs.wisc.edu}

\begin{document}

\begin{abstract}
Many datasets have multiple tables connected by key-foreign key dependencies.
Data scientists usually join all tables to bring in extra features from the so-called 
dimension tables. Unlike the statistical relational learning setting, such joins do 
not cause record duplications, which means regular IID models are typically used. Recent
work demonstrated the possibility of using foreign key features as representatives for the 
dimension tables' features and eliminating the latter a priori, potentially saving runtime
and effort of data scientists. However, the prior work was restricted to linear models 
and it established a dichotomy of when dimension tables are safe to discard due to extra
overfitting caused by the use of foreign key features. In this work, we revisit that question 
for two popular high capacity models: decision tree and SVM with RBF kernel. Our 
extensive empirical and simulation-based analyses show that these two classifers are surprisingly and 
counter-intuitively more robust to discarding dimension tables and face much \textit{less} extra overfitting 
than linear models. We provide intuitive explanations for their behavior and identify new open 
questions for further ML theoretical research. We also identify and resolve two key practical 
bottlenecks in using foreign key features.
\end{abstract}

\maketitle

\input{body}

\bibliographystyle{ACM-Reference-Format}
\bibliography{MLAvoidJoins}

\end{document}